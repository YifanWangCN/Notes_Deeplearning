{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707e39f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import math\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "import opt_utils #参见数据包或者在本文底部copy\n",
    "import testCase  #参见数据包或者在本文底部copy\n",
    "\n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b0a3c8",
   "metadata": {},
   "source": [
    "### 这个部分所有的算法的目的只有一个， 加快梯度下降算法的计算速度\n",
    "\n",
    "首先用到的方法是mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7c45097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X,Y,mini_batch_size=64,seed=0):\n",
    "    \"\"\"\n",
    "    从（X，Y）中创建一个随机的mini-batch列表\n",
    "    \n",
    "    参数：\n",
    "        X - 输入数据，维度为(输入节点数量，样本的数量)\n",
    "        Y - 对应的是X的标签，【1 | 0】（蓝|红），维度为(1,样本的数量)\n",
    "        mini_batch_size - 每个mini-batch的样本数量\n",
    "    返回：\n",
    "        mini-bacthes - 一个同步列表，维度为（mini_batch_X,mini_batch_Y）\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed) #指定随机种子\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "    \n",
    "    #第一步：打乱顺序\n",
    "    permutation = list(np.random.permutation(m)) #它会返回一个长度为m的随机数组，且里面的数是0到m-1，在这里就是生成随机索引\n",
    "    shuffled_X = X[:,permutation]                #将每一列的数据按permutation的顺序来重新排列。\n",
    "    shuffled_Y = Y[:,permutation].reshape((1,m))\n",
    "    \n",
    "    #第二步：分割，即mini-batch操作\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) #把你的训练集分割成多少份\n",
    "    \n",
    "    for k in range(0,num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:,k * mini_batch_size:(k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:,k * mini_batch_size:(k+1)*mini_batch_size]\n",
    "        \n",
    "        mini_batch = (mini_batch_X,mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "        \n",
    "    #这里会有一个疑问既然 math.floor（）是返回下舍整数，那么如果m不是mini-batch-size的整数倍的时候，最后多出来的那一组数据如何处理\n",
    "    #接下来的方法就是对该部分数据的处理\n",
    "    \n",
    "    if m % mini_batch_size != 0:\n",
    "        \n",
    "        #获取最后剩余的部分\n",
    "        \n",
    "        mini_batch_X = shuffled_X[:,mini_batch_size * num_complete_minibatches:]#其实可以发现这一步是非常直觉的\n",
    "        mini_batch_Y = shuffled_Y[:,mini_batch_size * num_complete_minibatches:]\n",
    "        \n",
    "        mini_batch = (mini_batch_X,mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "        \n",
    "        #可以理解返回了每一个batch的一个pair（X,Y）\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e4d8ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对permutation函数的测试\n",
    "bb = np.random.permutation(10) #返回一个长度为m的随机数组\n",
    "aa = list(np.random.permutation(10))\n",
    "# print(bb)\n",
    "# print(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff75d554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------测试random_mini_batches-------------\n",
      "第1个mini_batch_X 的维度为： (12288, 64)\n",
      "第1个mini_batch_Y 的维度为： (1, 64)\n",
      "第2个mini_batch_X 的维度为： (12288, 64)\n",
      "第2个mini_batch_Y 的维度为： (1, 64)\n",
      "第3个mini_batch_X 的维度为： (12288, 20)\n",
      "第3个mini_batch_Y 的维度为： (1, 20)\n"
     ]
    }
   ],
   "source": [
    "#测试random_mini_batches\n",
    "print(\"-------------测试random_mini_batches-------------\")\n",
    "X_assess,Y_assess,mini_batch_size = testCase.random_mini_batches_test_case()\n",
    "mini_batches = random_mini_batches(X_assess,Y_assess,mini_batch_size)\n",
    "\n",
    "print(\"第1个mini_batch_X 的维度为：\",mini_batches[0][0].shape)\n",
    "print(\"第1个mini_batch_Y 的维度为：\",mini_batches[0][1].shape)\n",
    "print(\"第2个mini_batch_X 的维度为：\",mini_batches[1][0].shape)\n",
    "print(\"第2个mini_batch_Y 的维度为：\",mini_batches[1][1].shape)\n",
    "print(\"第3个mini_batch_X 的维度为：\",mini_batches[2][0].shape)\n",
    "print(\"第3个mini_batch_Y 的维度为：\",mini_batches[2][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d03ef0",
   "metadata": {},
   "source": [
    "### 包含动量的梯度下降（Momentum gd）\n",
    "\n",
    "由于小批量梯度下降只看到了一个子集的参数更新，更新的方向有一定的差异，所以小批量梯度下降的路径将“振荡地”走向收敛，使用动量可以减少这些振荡，动量考虑了过去的梯度以平滑更新。其实本质便是对之前的梯度进行指数加权平均，降低一个维度的震荡幅度。\n",
    "\n",
    "既然我们要影响梯度的方向，而梯度需要使用到dW和db，那么我们就要建立一个和dW和db相同结构的变量来影响他们，我们现在来进行初始化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c347594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters):\n",
    "    \"\"\"\n",
    "    初始化速度，velocity是一个字典：\n",
    "        - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "        - values:与相应的梯度/参数维度相同的值为零的矩阵。\n",
    "    参数：\n",
    "        parameters - 一个字典，包含了以下参数：\n",
    "            parameters[\"W\" + str(l)] = Wl\n",
    "            parameters[\"b\" + str(l)] = bl\n",
    "    返回:\n",
    "        v - 一个字典变量，包含了以下参数：\n",
    "            v[\"dW\" + str(l)] = dWl的速度\n",
    "            v[\"db\" + str(l)] = dbl的速度\n",
    "    \n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 #神经网络的层数\n",
    "    v = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
    "        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0b9902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 其实就是在原有的更新参数函数中加了一个指数加权平均权重的算法\n",
    "def update_parameters_with_momentun(parameters,grads,v,beta,learning_rate):\n",
    "    \"\"\"\n",
    "    使用动量更新参数\n",
    "    参数：\n",
    "        parameters - 一个字典类型的变量，包含了以下字段：\n",
    "            parameters[\"W\" + str(l)] = Wl\n",
    "            parameters[\"b\" + str(l)] = bl\n",
    "        grads - 一个包含梯度值的字典变量，具有以下字段：\n",
    "            grads[\"dW\" + str(l)] = dWl\n",
    "            grads[\"db\" + str(l)] = dbl\n",
    "        v - 包含当前速度的字典变量，具有以下字段：\n",
    "            v[\"dW\" + str(l)] = ...\n",
    "            v[\"db\" + str(l)] = ...\n",
    "        beta - 超参数，动量，实数\n",
    "        learning_rate - 学习率，实数\n",
    "    返回：\n",
    "        parameters - 更新后的参数字典\n",
    "        v - 包含了更新后的速度变量\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 \n",
    "    for l in range(L):\n",
    "        \n",
    "        #计算速度\n",
    "        v[\"dW\" + str(l + 1)] = beta * v[\"dW\" + str(l + 1)] + (1 - beta) * grads[\"dW\" + str(l + 1)]\n",
    "        v[\"db\" + str(l + 1)] = beta * v[\"db\" + str(l + 1)] + (1 - beta) * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "        #更新参数\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v[\"db\" + str(l + 1)]\n",
    "    \n",
    "    return parameters,v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b55dd2b",
   "metadata": {},
   "source": [
    "### Adam算法\n",
    "Adam算法是训练神经网络中最有效的算法之一，它是RMSProp算法与Momentum算法的结合体。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4f4ebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters):\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
    "        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
    "        \n",
    "        s[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
    "        s[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
    "    \n",
    "    return (v,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773fa396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来就是见证奇迹的时刻\n",
    "def update_parameters_with_adam(parameters,grads,v,s,t,learning_rate=0.01,beta1=0.9,beta2=0.999,epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    使用Adam更新参数\n",
    "    \n",
    "    参数：\n",
    "        parameters - 包含了以下字段的字典：\n",
    "            parameters['W' + str(l)] = Wl\n",
    "            parameters['b' + str(l)] = bl\n",
    "        grads - 包含了梯度值的字典，有以下key值：\n",
    "            grads['dW' + str(l)] = dWl\n",
    "            grads['db' + str(l)] = dbl\n",
    "        v - Adam的变量，第一个梯度的移动平均值，是一个字典类型的变量\n",
    "        s - Adam的变量，平方梯度的移动平均值，是一个字典类型的变量\n",
    "        t - 当前迭代的次数\n",
    "        learning_rate - 学习率\n",
    "        beta1 - 动量，超参数,用于第一阶段，使得曲线的Y值不从0开始（参见天气数据的那个图）\n",
    "        beta2 - RMSprop的一个参数，超参数\n",
    "        epsilon - 防止除零操作（分母为0）\n",
    "    \n",
    "    返回：\n",
    "        parameters - 更新后的参数\n",
    "        v - 第一个梯度的移动平均值，是一个字典类型的变量\n",
    "        s - 平方梯度的移动平均值，是一个字典类型的变量\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2\n",
    "    v_corrected = {} #偏差修正后的值\n",
    "    s_corrected = {} #偏差修正后的值\n",
    "    \n",
    "    for l in range(L):\n",
    "        #梯度的移动平均值,输入：\"v , grads , beta1\",输出：\" v \"\n",
    "        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads[\"dW\" + str(l + 1)]\n",
    "        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "        #计算第一阶段的偏差修正后的估计值，输入\"v , beta1 , t\" , 输出：\"v_corrected\"\n",
    "        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta1,t))\n",
    "        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta1,t))\n",
    "    \n",
    "        #计算平方梯度的移动平均值，输入：\"s, grads , beta2\"，输出：\"s\"\n",
    "        s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.square(grads[\"dW\" + str(l + 1)])\n",
    "        s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.square(grads[\"db\" + str(l + 1)])\n",
    "         \n",
    "        #计算第二阶段的偏差修正后的估计值，输入：\"s , beta2 , t\"，输出：\"s_corrected\"\n",
    "        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2,t))\n",
    "        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2,t))\n",
    "        \n",
    "        #更新参数，输入: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". 输出: \"parameters\".\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * (v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s_corrected[\"dW\" + str(l + 1)] + epsilon))\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * (v_corrected[\"db\" + str(l + 1)] / np.sqrt(s_corrected[\"db\" + str(l + 1)] + epsilon))\n",
    "    \n",
    "    return (parameters,v,s)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
